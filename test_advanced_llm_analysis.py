#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced LLM Analysis æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å’Œè°ƒè¯•LLMåˆ†æåŠŸèƒ½
"""

import sys
import time
import json
import pandas as pd
import requests
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LLMAnalysisTester:
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        # ä½¿ç”¨é»˜è®¤é…ç½®
        self.config = {
            'url': 'https://agents.dyna.ai/openapi/v1/conversation/dialog/',
            'robot_key': 'AcZ%2FQzIk8m6UV0uNkXi3HO1pJPI%3D',
            'robot_token': 'MTc1MjEzMDE5Njc3NQp2SE5aZU92SFUvT1JwSVMvaFN3S3Jza1BlU1U9',
            'username': 'edison.chu@dyna.ai'
        }
        
        print("ğŸš€ Advanced LLM Analysis æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“¡ API URL: {self.config['url']}")
        print(f"ğŸ‘¤ ç”¨æˆ·å: {self.config['username']}")
        print("-" * 60)
    
    def test_api_connection(self):
        """æµ‹è¯•APIè¿æ¥"""
        print("\nğŸ” æµ‹è¯•1: APIè¿æ¥æµ‹è¯•")
        
        test_prompt = "è¯·å›ç­”ï¼šä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚"
        
        try:
            start_time = time.time()
            result = self.send_simple_request(test_prompt)
            elapsed_time = time.time() - start_time
            
            print(f"âœ… APIè¿æ¥æˆåŠŸï¼")
            print(f"â±ï¸ å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“„ å“åº”å†…å®¹: {result[:200]}...")
            return True
            
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def test_analysis_performance(self):
        """æµ‹è¯•åˆ†ææ€§èƒ½"""
        print("\nğŸ” æµ‹è¯•2: åˆ†ææ€§èƒ½æµ‹è¯•")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = {
            'reference': '{"type": "markdown", "data": "| ID | äº§å“ | åº“å­˜ | ä»·æ ¼ |\\n|:---|:-----|:-----|:-----|\\n| LL-C30210 | 185/65R15 COMPASAL BLAZER HP 88H | 8 | $1142 |\\n", "desc": "æ‰¾åˆ°1ä¸ªäº§å“ï¼Œä»·æ ¼$1142"}',
            'generated': '{"type": "markdown", "data": "| ID | äº§å“ | åº“å­˜ | ä»·æ ¼ |\\n|:---|:-----|:-----|:-----|\\n| LL-C30210 | 185/65R15 COMPASAL BLAZER HP 88H | 8 | $1142 |\\n", "desc": "æ‰¾åˆ°1ä¸ªäº§å“ï¼Œä»·æ ¼$1142"}'
        }
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„åˆ†æ
        analysis_types = [
            ("comprehensive", "å…¨é¢åˆ†æ"),
            ("tire_business", "ä¸šåŠ¡åˆ†æ"),
            ("agent_comparison", "å¯¹æ¯”åˆ†æ")
        ]
        
        results = {}
        
        for analysis_type, chinese_name in analysis_types:
            print(f"\nğŸ§  æµ‹è¯• {chinese_name} ({analysis_type})...")
            
            try:
                start_time = time.time()
                
                # åˆ›å»ºæç¤ºè¯
                prompt = self.create_evaluation_prompt(
                    test_data['reference'], 
                    test_data['generated'], 
                    analysis_type
                )
                
                # å‘é€è¯·æ±‚
                result = self.send_simple_request(prompt)
                elapsed_time = time.time() - start_time
                
                results[analysis_type] = {
                    'success': True,
                    'time': elapsed_time,
                    'result_length': len(result),
                    'result_preview': result[:300] + "..." if len(result) > 300 else result
                }
                
                print(f"âœ… {chinese_name} æˆåŠŸï¼")
                print(f"â±ï¸ è€—æ—¶: {elapsed_time:.2f}ç§’")
                print(f"ğŸ“„ ç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")
                
            except Exception as e:
                results[analysis_type] = {
                    'success': False,
                    'error': str(e),
                    'time': 0
                }
                print(f"âŒ {chinese_name} å¤±è´¥: {str(e)}")
        
        return results
    
    def test_batch_analysis(self, sample_size=3):
        """æµ‹è¯•æ‰¹é‡åˆ†æ"""
        print(f"\nğŸ” æµ‹è¯•3: æ‰¹é‡åˆ†ææµ‹è¯• ({sample_size}ä¸ªæ ·æœ¬)")
        
        # åˆ›å»ºæµ‹è¯•DataFrame
        test_df = pd.DataFrame({
            'åœºæ™¯': [f'æµ‹è¯•åœºæ™¯{i+1}' for i in range(sample_size)],
            'å‚è€ƒç­”æ¡ˆ': [f'è¿™æ˜¯å‚è€ƒç­”æ¡ˆ{i+1}ï¼ŒåŒ…å«è½®èƒè§„æ ¼185/65R15ï¼Œä»·æ ¼$1142' for i in range(sample_size)],
            'ç”Ÿæˆç­”æ¡ˆ1': [f'è¿™æ˜¯ç”Ÿæˆç­”æ¡ˆ{i+1}ï¼ŒåŒ…å«è½®èƒè§„æ ¼185/65R15ï¼Œä»·æ ¼$1142' for i in range(sample_size)]
        })
        
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(test_df)} è¡Œ")
        
        try:
            # å¯¼å…¥åˆ†æå™¨
            from advanced_llm_analyzer import AdvancedLLMAnalyzer
            
            # åˆ›å»ºåˆ†æå™¨
            analyzer = AdvancedLLMAnalyzer(self.config)
            
            # å®šä¹‰è¿›åº¦å›è°ƒ
            def progress_callback(current, total):
                progress = (current / total) * 100
                print(f"ğŸ“ˆ è¿›åº¦: {current}/{total} ({progress:.1f}%)")
            
            # æ‰§è¡Œåˆ†æ
            start_time = time.time()
            result_df = analyzer.batch_analyze_dataframe(
                test_df, 
                'å‚è€ƒç­”æ¡ˆ', 
                'ç”Ÿæˆç­”æ¡ˆ1',
                'comprehensive',
                progress_callback=progress_callback
            )
            total_time = time.time() - start_time
            
            print(f"âœ… æ‰¹é‡åˆ†ææˆåŠŸï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¹³å‡æ¯æ ·æœ¬: {total_time/sample_size:.2f}ç§’")
            print(f"ğŸ“„ ç»“æœåˆ—æ•°: {len(result_df.columns)}")
            
            # æ˜¾ç¤ºç»“æœåˆ—
            llm_columns = [col for col in result_df.columns if col.startswith('llm_')]
            print(f"ğŸ§  LLMåˆ†æåˆ—: {len(llm_columns)} ä¸ª")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def send_simple_request(self, prompt, timeout=30):
        """å‘é€ç®€å•è¯·æ±‚"""
        url = self.config['url']
        headers = {
            'cybertron-robot-key': self.config['robot_key'],
            'cybertron-robot-token': self.config['robot_token'],
            'Content-Type': 'application/json'
        }
        data = {
            "username": self.config['username'],
            "question": prompt,
            "segment_code": "qa_analysis"
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=timeout)
        response.raise_for_status()
        result = response.json()
        
        if result.get('code') == '000000':
            if 'data' in result and 'answer' in result['data']:
                return result['data']['answer']
            return str(result.get('data', {}))
        else:
            raise Exception(f"API Error: {result.get('code')} - {result.get('message')}")
    
    def create_evaluation_prompt(self, reference: str, generated: str, evaluation_type: str = "comprehensive") -> str:
        """åˆ›å»ºè¯„ä¼°æç¤ºè¯"""
        if evaluation_type == "comprehensive":
            return f"""
ä½œä¸ºAIè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼š

å‚è€ƒç­”æ¡ˆï¼š{reference}
ç”Ÿæˆç­”æ¡ˆï¼š{generated}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼ˆ0-10åˆ†ï¼‰ï¼š
1. äº‹å®å‡†ç¡®æ€§
2. è¯­ä¹‰ä¸€è‡´æ€§
3. ä¸šåŠ¡é€»è¾‘ç¬¦åˆæ€§
4. å›ç­”å®Œæ•´æ€§
5. ä¿¡æ¯ç›¸å…³æ€§

è¯·ç”¨JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœã€‚
"""
        elif evaluation_type == "tire_business":
            return f"""
ä½œä¸ºè½®èƒä¸šåŠ¡ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„ä¸šåŠ¡å‡†ç¡®æ€§ï¼š

å‚è€ƒç­”æ¡ˆï¼š{reference}
ç”Ÿæˆç­”æ¡ˆï¼š{generated}

è¯·ä»è½®èƒä¸šåŠ¡è§’åº¦è¯„ä¼°ï¼š
1. è½®èƒè§„æ ¼å‡†ç¡®æ€§
2. ä»·æ ¼å‡†ç¡®æ€§
3. åº“å­˜å‡†ç¡®æ€§

è¯·ç”¨JSONæ ¼å¼è¿”å›ç»“æœã€‚
"""
        elif evaluation_type == "agent_comparison":
            return f"""
è¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡ï¼š

å‚è€ƒç­”æ¡ˆï¼š{reference}
ç”Ÿæˆç­”æ¡ˆï¼š{generated}

è¯·åˆ†æï¼š
1. å“ªä¸ªç­”æ¡ˆæ›´å¥½
2. å…·ä½“å·®å¼‚åœ¨å“ªé‡Œ
3. æ”¹è¿›å»ºè®®

è¯·ç”¨JSONæ ¼å¼è¿”å›å¯¹æ¯”ç»“æœã€‚
"""
        else:
            return self.create_evaluation_prompt(reference, generated, "comprehensive")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹è¿è¡Œ Advanced LLM Analysis å®Œæ•´æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•1: APIè¿æ¥
        connection_ok = self.test_api_connection()
        if not connection_ok:
            print("\nâŒ APIè¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
            return False
        
        # æµ‹è¯•2: åˆ†ææ€§èƒ½
        performance_results = self.test_analysis_performance()
        
        # æµ‹è¯•3: æ‰¹é‡åˆ†æ
        batch_ok = self.test_batch_analysis(sample_size=2)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        print("\nğŸ“‹ æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print("=" * 60)
        print(f"âœ… APIè¿æ¥æµ‹è¯•: {'é€šè¿‡' if connection_ok else 'å¤±è´¥'}")
        
        if performance_results:
            for analysis_type, result in performance_results.items():
                status = "é€šè¿‡" if result['success'] else "å¤±è´¥"
                time_info = f"({result['time']:.2f}ç§’)" if result['success'] else f"({result.get('error', 'æœªçŸ¥é”™è¯¯')})"
                print(f"âœ… {analysis_type}åˆ†æ: {status} {time_info}")
        
        print(f"âœ… æ‰¹é‡åˆ†ææµ‹è¯•: {'é€šè¿‡' if batch_ok else 'å¤±è´¥'}")
        
        # æ€§èƒ½å»ºè®®
        if performance_results:
            avg_time = sum(r['time'] for r in performance_results.values() if r['success']) / len([r for r in performance_results.values() if r['success']])
            print(f"\nğŸ’¡ æ€§èƒ½åˆ†æ:")
            print(f"   å¹³å‡å•æ¬¡APIè°ƒç”¨æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"   10ä¸ªæ ·æœ¬å…¨é¢åˆ†æé¢„è®¡æ—¶é—´: {10 * 3 * avg_time:.0f}ç§’")
            print(f"   50ä¸ªæ ·æœ¬å…¨é¢åˆ†æé¢„è®¡æ—¶é—´: {50 * 3 * avg_time:.0f}ç§’")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Advanced LLM Analysis åŠŸèƒ½æµ‹è¯•è„šæœ¬")
    print("ğŸ“… æµ‹è¯•æ—¶é—´:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    try:
        tester = LLMAnalysisTester()
        tester.run_all_tests()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 