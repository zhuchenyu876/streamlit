#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import logging
from typing import Dict, List, Optional, Tuple
import pandas as pd
from tqdm import tqdm
import json
import requests
import time
from requests.exceptions import RequestException, Timeout, ConnectionError
import numpy as np
from datetime import datetime
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
import uuid

# ğŸ§  å¢å¼ºç‰ˆLLMåˆ†æå™¨ - æ”¯æŒæ¨¡æ‹Ÿæ¨¡å¼å’ŒçœŸå®API
# 
# æ¨¡å¼è¯´æ˜ï¼š
# 1. DEMO_MODE = True  -> ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤ºåŠŸèƒ½ï¼ˆæ— éœ€API Keyï¼‰
# 2. DEMO_MODE = False -> ä½¿ç”¨çœŸå®LLM APIåˆ†æï¼ˆéœ€è¦é…ç½®API Keyï¼‰

import os
import json
import random
import asyncio
import aiohttp
from typing import Dict, List, Any, Optional
from datetime import datetime

# ğŸ¯ æ¨¡æ‹Ÿæ¨¡å¼å¼€å…³ - è®¾ç½®ä¸ºTrueå¯æ— éœ€APIä½“éªŒåŠŸèƒ½
DEMO_MODE = True  # è®¾ç½®ä¸º False ä»¥ä½¿ç”¨çœŸå®API

# ğŸ”§ æ”¯æŒçš„LLMæœåŠ¡æä¾›å•†
SUPPORTED_PROVIDERS = {
    'openai': {
        'name': 'OpenAI',
        'models': ['gpt-4o-mini', 'gpt-3.5-turbo', 'gpt-4'],
        'default_model': 'gpt-4o-mini'
    },
    'azure': {
        'name': 'Azure OpenAI',
        'models': ['gpt-4', 'gpt-35-turbo'],
        'default_model': 'gpt-4'
    },
    'ollama': {
        'name': 'Ollama (æœ¬åœ°)',
        'models': ['llama2:7b', 'llama2:13b', 'codellama:7b'],
        'default_model': 'llama2:7b'
    }
}

class AdvancedLLMAnalyzer:
    """
    å¢å¼ºç‰ˆLLMåˆ†æå™¨ï¼Œæä¾›å¤šç»´åº¦å‡†ç¡®æ€§è¯„ä¼°å’Œagentè¯„æµ‹åŠŸèƒ½
    """
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆLLMåˆ†æå™¨
        
        Args:
            config (dict): åˆ†æå™¨é…ç½®å‚æ•°
        """
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        
        self.config = config or {}
        
        # å®šä¹‰è¯„ä¼°ç»´åº¦
        self.evaluation_dimensions = {
            'factual_accuracy': 'äº‹å®å‡†ç¡®æ€§',
            'semantic_consistency': 'è¯­ä¹‰ä¸€è‡´æ€§', 
            'business_logic_compliance': 'ä¸šåŠ¡é€»è¾‘ç¬¦åˆæ€§',
            'response_completeness': 'å›ç­”å®Œæ•´æ€§',
            'information_relevance': 'ä¿¡æ¯ç›¸å…³æ€§',
            'language_quality': 'è¯­è¨€è´¨é‡',
            'user_intent_fulfillment': 'ç”¨æˆ·æ„å›¾æ»¡è¶³åº¦',
            'technical_accuracy': 'æŠ€æœ¯å‡†ç¡®æ€§',
            'context_understanding': 'ä¸Šä¸‹æ–‡ç†è§£',
            'professional_tone': 'ä¸“ä¸šç¨‹åº¦'
        }
        
        # è½®èƒä¸šåŠ¡ä¸“é—¨çš„è¯„ä¼°ç»´åº¦
        self.tire_business_dimensions = {
            'tire_spec_accuracy': 'è½®èƒè§„æ ¼å‡†ç¡®æ€§',
            'price_accuracy': 'ä»·æ ¼å‡†ç¡®æ€§',
            'stock_accuracy': 'åº“å­˜å‡†ç¡®æ€§',
            'brand_consistency': 'å“ç‰Œä¸€è‡´æ€§',
            'service_info_accuracy': 'æœåŠ¡ä¿¡æ¯å‡†ç¡®æ€§',
            'sales_process_compliance': 'é”€å”®æµç¨‹ç¬¦åˆæ€§'
        }

    def create_evaluation_prompt(self, reference: str, generated: str, evaluation_type: str = "comprehensive") -> str:
        """
        åˆ›å»ºè¯„ä¼°æç¤ºè¯
        
        Args:
            reference (str): å‚è€ƒç­”æ¡ˆ
            generated (str): ç”Ÿæˆç­”æ¡ˆ
            evaluation_type (str): è¯„ä¼°ç±»å‹ ('comprehensive', 'tire_business', 'agent_comparison')
            
        Returns:
            str: è¯„ä¼°æç¤ºè¯
        """
        if evaluation_type == "comprehensive":
            return f"""
ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„AIè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è½®èƒé”€å”®å¯¹è¯è¿›è¡Œå…¨é¢çš„è´¨é‡è¯„ä¼°ã€‚

**å‚è€ƒç­”æ¡ˆï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰ï¼š**
{reference}

**ç”Ÿæˆç­”æ¡ˆï¼ˆå¾…è¯„ä¼°ç­”æ¡ˆï¼‰ï¼š**
{generated}

**è¯„ä¼°è¦æ±‚ï¼š**
è¯·ä»ä»¥ä¸‹10ä¸ªç»´åº¦å¯¹ç”Ÿæˆç­”æ¡ˆè¿›è¡Œè¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦æ‰“åˆ†0-10åˆ†ï¼Œå¹¶æä¾›è¯¦ç»†è¯´æ˜ï¼š

1. **äº‹å®å‡†ç¡®æ€§** (0-10åˆ†)ï¼šä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯
2. **è¯­ä¹‰ä¸€è‡´æ€§** (0-10åˆ†)ï¼šè¯­ä¹‰æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´
3. **ä¸šåŠ¡é€»è¾‘ç¬¦åˆæ€§** (0-10åˆ†)ï¼šæ˜¯å¦ç¬¦åˆè½®èƒé”€å”®ä¸šåŠ¡é€»è¾‘
4. **å›ç­”å®Œæ•´æ€§** (0-10åˆ†)ï¼šå›ç­”æ˜¯å¦å®Œæ•´ï¼Œæ²¡æœ‰é—æ¼å…³é”®ä¿¡æ¯
5. **ä¿¡æ¯ç›¸å…³æ€§** (0-10åˆ†)ï¼šä¿¡æ¯æ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³
6. **è¯­è¨€è´¨é‡** (0-10åˆ†)ï¼šè¯­è¨€è¡¨è¾¾æ˜¯å¦æ¸…æ™°ã€ä¸“ä¸š
7. **ç”¨æˆ·æ„å›¾æ»¡è¶³åº¦** (0-10åˆ†)ï¼šæ˜¯å¦æ»¡è¶³ç”¨æˆ·çš„å®é™…éœ€æ±‚
8. **æŠ€æœ¯å‡†ç¡®æ€§** (0-10åˆ†)ï¼šæŠ€æœ¯ç»†èŠ‚æ˜¯å¦å‡†ç¡®
9. **ä¸Šä¸‹æ–‡ç†è§£** (0-10åˆ†)ï¼šæ˜¯å¦æ­£ç¡®ç†è§£å¯¹è¯ä¸Šä¸‹æ–‡
10. **ä¸“ä¸šç¨‹åº¦** (0-10åˆ†)ï¼šæ˜¯å¦ä½“ç°ä¸“ä¸šçš„é”€å”®æœåŠ¡æ°´å¹³

**ç‰¹åˆ«å…³æ³¨ï¼š**
- å¦‚æœæ˜¯JSONæ ¼å¼å›ç­”ï¼Œæ£€æŸ¥JSONç»“æ„æ˜¯å¦æ­£ç¡®
- è½®èƒè§„æ ¼ä¿¡æ¯ï¼ˆå¦‚185/65R15ï¼‰æ˜¯å¦å‡†ç¡®
- ä»·æ ¼ä¿¡æ¯æ˜¯å¦æ­£ç¡®
- åº“å­˜ä¿¡æ¯æ˜¯å¦å‡†ç¡®
- å“ç‰Œä¿¡æ¯æ˜¯å¦ä¸€è‡´

**è¾“å‡ºæ ¼å¼ï¼š**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š

{{
    "factual_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "semantic_consistency": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "business_logic_compliance": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "response_completeness": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "information_relevance": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "language_quality": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "user_intent_fulfillment": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "technical_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "context_understanding": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "professional_tone": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "overall_score": X,
    "overall_assessment": "æ€»ä½“è¯„ä¼°è¯´æ˜",
    "key_issues": ["ä¸»è¦é—®é¢˜1", "ä¸»è¦é—®é¢˜2"],
    "recommendations": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
}}
"""
        
        elif evaluation_type == "tire_business":
            return f"""
ä½œä¸ºè½®èƒé”€å”®ä¸šåŠ¡ä¸“å®¶ï¼Œè¯·ä¸“é—¨è¯„ä¼°ä»¥ä¸‹è½®èƒé”€å”®å¯¹è¯çš„ä¸šåŠ¡å‡†ç¡®æ€§ã€‚

**å‚è€ƒç­”æ¡ˆï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰ï¼š**
{reference}

**ç”Ÿæˆç­”æ¡ˆï¼ˆå¾…è¯„ä¼°ç­”æ¡ˆï¼‰ï¼š**
{generated}

**ä¸“ä¸šä¸šåŠ¡è¯„ä¼°è¦æ±‚ï¼š**
è¯·ä»ä»¥ä¸‹6ä¸ªè½®èƒä¸šåŠ¡ä¸“é—¨ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œæ¯ä¸ªç»´åº¦æ‰“åˆ†0-10åˆ†ï¼š

1. **è½®èƒè§„æ ¼å‡†ç¡®æ€§** (0-10åˆ†)ï¼šè½®èƒè§„æ ¼ä¿¡æ¯æ˜¯å¦å‡†ç¡®
2. **ä»·æ ¼å‡†ç¡®æ€§** (0-10åˆ†)ï¼šä»·æ ¼ä¿¡æ¯æ˜¯å¦å‡†ç¡®
3. **åº“å­˜å‡†ç¡®æ€§** (0-10åˆ†)ï¼šåº“å­˜ä¿¡æ¯æ˜¯å¦å‡†ç¡®
4. **å“ç‰Œä¸€è‡´æ€§** (0-10åˆ†)ï¼šå“ç‰Œä¿¡æ¯æ˜¯å¦ä¸€è‡´
5. **æœåŠ¡ä¿¡æ¯å‡†ç¡®æ€§** (0-10åˆ†)ï¼šå®‰è£…ã€é…é€ç­‰æœåŠ¡ä¿¡æ¯æ˜¯å¦å‡†ç¡®
6. **é”€å”®æµç¨‹ç¬¦åˆæ€§** (0-10åˆ†)ï¼šæ˜¯å¦ç¬¦åˆGrupo Magnoçš„é”€å”®æµç¨‹

**è¾“å‡ºæ ¼å¼ï¼š**
{{
    "tire_spec_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "price_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "stock_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "brand_consistency": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "service_info_accuracy": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "sales_process_compliance": {{"score": X, "explanation": "è¯¦ç»†è¯´æ˜"}},
    "business_overall_score": X,
    "business_assessment": "ä¸šåŠ¡è¯„ä¼°è¯´æ˜"
}}
"""
        
        elif evaluation_type == "agent_comparison":
            return f"""
ä½œä¸ºAI Agentè¯„ä¼°ä¸“å®¶ï¼Œè¯·å¯¹è¿™ä¸¤ä¸ªç­”æ¡ˆè¿›è¡Œæ·±åº¦å¯¹æ¯”åˆ†æã€‚

**å‚è€ƒç­”æ¡ˆï¼ˆåŸºå‡†ç­”æ¡ˆï¼‰ï¼š**
{reference}

**ç”Ÿæˆç­”æ¡ˆï¼ˆå¾…è¯„ä¼°ç­”æ¡ˆï¼‰ï¼š**
{generated}

**Agentå¯¹æ¯”è¯„ä¼°è¦æ±‚ï¼š**
è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œæ·±åº¦å¯¹æ¯”åˆ†æï¼š

1. **ç­”æ¡ˆè´¨é‡å¯¹æ¯”**ï¼šå“ªä¸ªç­”æ¡ˆæ›´å¥½ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
2. **å‡†ç¡®æ€§å¯¹æ¯”**ï¼šå“ªä¸ªç­”æ¡ˆæ›´å‡†ç¡®ï¼Œå…·ä½“å·®å¼‚åœ¨å“ªé‡Œï¼Ÿ
3. **ç”¨æˆ·ä½“éªŒå¯¹æ¯”**ï¼šå“ªä¸ªç­”æ¡ˆç”¨æˆ·ä½“éªŒæ›´å¥½ï¼Ÿ
4. **ä¸šåŠ¡ä»·å€¼å¯¹æ¯”**ï¼šå“ªä¸ªç­”æ¡ˆæ›´æœ‰å•†ä¸šä»·å€¼ï¼Ÿ
5. **æ”¹è¿›ç©ºé—´åˆ†æ**ï¼šç”Ÿæˆç­”æ¡ˆè¿˜æœ‰å“ªäº›æ”¹è¿›ç©ºé—´ï¼Ÿ

**è¾“å‡ºæ ¼å¼ï¼š**
{{
    "quality_comparison": "è´¨é‡å¯¹æ¯”åˆ†æ",
    "accuracy_comparison": "å‡†ç¡®æ€§å¯¹æ¯”åˆ†æ", 
    "user_experience_comparison": "ç”¨æˆ·ä½“éªŒå¯¹æ¯”åˆ†æ",
    "business_value_comparison": "ä¸šåŠ¡ä»·å€¼å¯¹æ¯”åˆ†æ",
    "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
    "winner": "reference/generated/tie",
    "confidence_level": "high/medium/low",
    "detailed_analysis": "è¯¦ç»†åˆ†æè¯´æ˜"
}}
"""
        
        else:
            return self.create_evaluation_prompt(reference, generated, "comprehensive")

    def send_evaluation_request(self, prompt: str, timeout: int = 120, max_retries: int = 3) -> str:
        """
        å‘é€è¯„ä¼°è¯·æ±‚åˆ°API - ä¼˜åŒ–504é”™è¯¯å¤„ç†
        
        Args:
            prompt (str): è¯„ä¼°æç¤º
            timeout (int): è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰- å¢åŠ åˆ°120ç§’åº”å¯¹504é”™è¯¯
            max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•° - ä¿æŒ3æ¬¡
            
        Returns:
            str: APIå“åº”ç»“æœ
        """
        
        for attempt in range(max_retries):
            try:
                # è®°å½•è¯·æ±‚å¼€å§‹
                self.logger.info(f"å‘é€APIè¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries})")
                
                response = requests.post(
                    self.config['url'],
                    json={
                        'username': self.config['username'],
                        'question': prompt,
                        'segment_code': str(uuid.uuid4())
                    },
                    timeout=timeout,
                    headers={
                        'cybertron-robot-key': self.config['robot_key'],
                        'cybertron-robot-token': self.config['robot_token'],
                        'Content-Type': 'application/json'
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get('code') == '000000':
                        if 'data' in result and 'answer' in result['data']:
                            content = result['data']['answer']
                            self.logger.info(f"APIè¯·æ±‚æˆåŠŸ (å°è¯• {attempt + 1})")
                            return content
                        else:
                            content = str(result.get('data', {}))
                            self.logger.info(f"APIè¯·æ±‚æˆåŠŸ (å°è¯• {attempt + 1})")
                            return content
                    else:
                        error_msg = result.get('message', 'Unknown error')
                        self.logger.warning(f"APIè¿”å›é”™è¯¯: {error_msg}")
                        raise Exception(f"APIé”™è¯¯: {error_msg}")
                else:
                    self.logger.warning(f"HTTPé”™è¯¯: {response.status_code}")
                    raise Exception(f"HTTPé”™è¯¯: {response.status_code}")
                    
            except requests.exceptions.Timeout:
                self.logger.warning(f"å°è¯• {attempt + 1} å¤±è´¥: è¯·æ±‚è¶…æ—¶ ({timeout}ç§’)")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3  # 504é”™è¯¯å¢åŠ ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"APIè¯·æ±‚è¶…æ—¶ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    
            except requests.exceptions.ConnectionError as e:
                self.logger.warning(f"å°è¯• {attempt + 1} å¤±è´¥: è¿æ¥é”™è¯¯ - {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # è¿æ¥é”™è¯¯ç­‰å¾…æ›´é•¿æ—¶é—´
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise Exception(f"APIè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    
            except Exception as e:
                self.logger.warning(f"å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                
                # 504é”™è¯¯ç‰¹æ®Šå¤„ç†
                if "504" in str(e):
                    self.logger.warning("æ£€æµ‹åˆ°504ç½‘å…³è¶…æ—¶é”™è¯¯")
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 5  # 504é”™è¯¯ç­‰å¾…æ›´é•¿æ—¶é—´
                        self.logger.info(f"504é”™è¯¯é‡è¯•ç­–ç•¥ï¼šç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        raise Exception(f"504ç½‘å…³è¶…æ—¶ï¼Œå·²é‡è¯• {max_retries} æ¬¡ã€‚å»ºè®®ï¼š1) å‡å°‘æ ·æœ¬æ•°é‡ 2) ä½¿ç”¨æ›´ç®€å•çš„åˆ†æç±»å‹")
                else:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 3
                        self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    else:
                        raise Exception(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        
        raise Exception(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIæœåŠ¡çŠ¶æ€")

    def parse_evaluation_result(self, result_text: str, evaluation_type: str = "comprehensive") -> Dict:
        """
        è§£æè¯„ä¼°ç»“æœ
        
        Args:
            result_text (str): è¯„ä¼°ç»“æœæ–‡æœ¬
            evaluation_type (str): è¯„ä¼°ç±»å‹
            
        Returns:
            Dict: è§£æåçš„è¯„ä¼°ç»“æœ
        """
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if result_text.strip().startswith('{') and result_text.strip().endswith('}'):
                return json.loads(result_text)
            
            # å¦‚æœä¸æ˜¯çº¯JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            
            # è§£æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ç»“æœ
            return self._get_error_evaluation_result(evaluation_type)
            
        except json.JSONDecodeError:
            self.logger.error(f"Failed to parse evaluation result: {result_text}")
            return self._get_error_evaluation_result(evaluation_type)

    def _get_error_evaluation_result(self, evaluation_type: str = "comprehensive") -> Dict:
        """
        è·å–é”™è¯¯è¯„ä¼°ç»“æœ
        
        Args:
            evaluation_type (str): è¯„ä¼°ç±»å‹
            
        Returns:
            Dict: é”™è¯¯è¯„ä¼°ç»“æœ
        """
        if evaluation_type == "comprehensive":
            return {
                "factual_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "semantic_consistency": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "business_logic_compliance": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "response_completeness": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "information_relevance": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "language_quality": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "user_intent_fulfillment": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "technical_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "context_understanding": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "professional_tone": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "overall_score": 0,
                "overall_assessment": "è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                "key_issues": ["è¯„ä¼°å¤±è´¥"],
                "recommendations": ["é‡æ–°è¯„ä¼°"]
            }
        elif evaluation_type == "tire_business":
            return {
                "tire_spec_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "price_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "stock_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "brand_consistency": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "service_info_accuracy": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "sales_process_compliance": {"score": 0, "explanation": "è¯„ä¼°å¤±è´¥"},
                "business_overall_score": 0,
                "business_assessment": "è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"
            }
        elif evaluation_type == "agent_comparison":
            return {
                "quality_comparison": "è¯„ä¼°å¤±è´¥",
                "accuracy_comparison": "è¯„ä¼°å¤±è´¥",
                "user_experience_comparison": "è¯„ä¼°å¤±è´¥", 
                "business_value_comparison": "è¯„ä¼°å¤±è´¥",
                "improvement_suggestions": ["é‡æ–°è¯„ä¼°"],
                "winner": "unknown",
                "confidence_level": "low",
                "detailed_analysis": "è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"
            }
        else:
            return {"error": "Unknown evaluation type"}

    def comprehensive_analyze(self, reference: str, generated: str) -> Dict:
        """
        å…¨é¢åˆ†æå•ä¸ªç­”æ¡ˆå¯¹
        
        Args:
            reference (str): å‚è€ƒç­”æ¡ˆ
            generated (str): ç”Ÿæˆç­”æ¡ˆ
            
        Returns:
            Dict: å…¨é¢åˆ†æç»“æœ
        """
        # 1. ç»¼åˆè¯„ä¼°
        comprehensive_prompt = self.create_evaluation_prompt(reference, generated, "comprehensive")
        comprehensive_result = self.send_evaluation_request(comprehensive_prompt)
        comprehensive_analysis = self.parse_evaluation_result(comprehensive_result, "comprehensive")
        
        # 2. ä¸šåŠ¡ä¸“é—¨è¯„ä¼°
        business_prompt = self.create_evaluation_prompt(reference, generated, "tire_business")
        business_result = self.send_evaluation_request(business_prompt)
        business_analysis = self.parse_evaluation_result(business_result, "tire_business")
        
        # 3. Agentå¯¹æ¯”è¯„ä¼°
        comparison_prompt = self.create_evaluation_prompt(reference, generated, "agent_comparison")
        comparison_result = self.send_evaluation_request(comparison_prompt)
        comparison_analysis = self.parse_evaluation_result(comparison_result, "agent_comparison")
        
        # åˆå¹¶ç»“æœ
        return {
            "comprehensive_analysis": comprehensive_analysis,
            "business_analysis": business_analysis,
            "comparison_analysis": comparison_analysis,
            "analysis_timestamp": datetime.now().isoformat()
        }

    def batch_analyze_dataframe(self, df: pd.DataFrame, ref_col: str, gen_col: str, 
                              analysis_type: str = "comprehensive",
                              progress_callback=None, pause_check_callback=None) -> pd.DataFrame:
        """
        æ‰¹é‡åˆ†æDataFrame
        
        Args:
            df (pd.DataFrame): æ•°æ®æ¡†
            ref_col (str): å‚è€ƒç­”æ¡ˆåˆ—å
            gen_col (str): ç”Ÿæˆç­”æ¡ˆåˆ—å
            analysis_type (str): åˆ†æç±»å‹
            progress_callback: è¿›åº¦å›è°ƒ
            pause_check_callback: æš‚åœæ£€æŸ¥å›è°ƒ
            
        Returns:
            pd.DataFrame: æ·»åŠ äº†åˆ†æç»“æœçš„æ•°æ®æ¡†
        """
        results = []
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc=f"æ‰§è¡Œ{analysis_type}åˆ†æ"):
            # å…ˆæ›´æ–°è¿›åº¦ï¼ˆæ˜¾ç¤ºå½“å‰æ­£åœ¨å¤„ç†çš„æ ·æœ¬ï¼‰
            if progress_callback:
                progress_callback(idx, len(df))
            
            # æ£€æŸ¥æš‚åœ
            if pause_check_callback and not pause_check_callback(0.95, f"æ­£åœ¨æ‰§è¡Œå¢å¼ºLLMåˆ†æ: {idx + 1}/{len(df)}"):
                import streamlit as st
                while st.session_state.get('analysis_paused', False):
                    time.sleep(1)
                    if not st.session_state.get('analysis_running', False):
                        # åˆ†æè¢«åœæ­¢ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ
                        if results:
                            self._add_partial_results_to_df(df, results, analysis_type)
                        return df
            
            try:
                # è®°å½•å¼€å§‹å¤„ç†
                self.logger.info(f"å¼€å§‹å¤„ç†æ ·æœ¬ {idx + 1}/{len(df)}")
                
                if analysis_type == "comprehensive":
                    result = self.comprehensive_analyze(str(row[ref_col]), str(row[gen_col]))
                elif analysis_type == "business_only":
                    prompt = self.create_evaluation_prompt(str(row[ref_col]), str(row[gen_col]), "tire_business")
                    result_text = self.send_evaluation_request(prompt)
                    result = self.parse_evaluation_result(result_text, "tire_business")
                elif analysis_type == "comparison_only":
                    prompt = self.create_evaluation_prompt(str(row[ref_col]), str(row[gen_col]), "agent_comparison")
                    result_text = self.send_evaluation_request(prompt)
                    result = self.parse_evaluation_result(result_text, "agent_comparison")
                else:
                    result = self.comprehensive_analyze(str(row[ref_col]), str(row[gen_col]))
                
                results.append(result)
                
                # è®°å½•å®Œæˆå¤„ç†
                self.logger.info(f"å®Œæˆå¤„ç†æ ·æœ¬ {idx + 1}/{len(df)}")
                
            except Exception as e:
                # å¤„ç†APIé”™è¯¯
                self.logger.error(f"å¤„ç†æ ·æœ¬ {idx + 1} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                
                # æ·»åŠ é”™è¯¯ç»“æœ
                error_result = self._get_error_evaluation_result(analysis_type)
                results.append(error_result)
                
                # å¦‚æœæ˜¯ä¸¥é‡é”™è¯¯ï¼Œè€ƒè™‘åœæ­¢åˆ†æ
                if "timeout" in str(e).lower() or "connection" in str(e).lower():
                    self.logger.error(f"æ£€æµ‹åˆ°ä¸¥é‡é”™è¯¯ï¼Œåœæ­¢åˆ†æ: {str(e)}")
                    break
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            time.sleep(0.5) # å‡å°‘è¯·æ±‚é—´éš”
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ˜¾ç¤ºå·²å®Œæˆçš„æ ·æœ¬ï¼‰
            if progress_callback:
                progress_callback(idx + 1, len(df))
            
            # æ¯10è¡Œè®°å½•ä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 10 == 0:
                self.logger.info(f"å·²å®Œæˆå¢å¼ºLLMåˆ†æ {idx + 1}/{len(df)} è¡Œ")
        
        # æ·»åŠ ç»“æœåˆ°DataFrame
        self._add_results_to_df(df, results, analysis_type)
        
        return df

    def _add_results_to_df(self, df: pd.DataFrame, results: List[Dict], analysis_type: str):
        """
        å°†åˆ†æç»“æœæ·»åŠ åˆ°DataFrame
        
        Args:
            df (pd.DataFrame): æ•°æ®æ¡†
            results (List[Dict]): åˆ†æç»“æœåˆ—è¡¨
            analysis_type (str): åˆ†æç±»å‹
        """
        if analysis_type == "comprehensive":
            for i, result in enumerate(results):
                # ç»¼åˆåˆ†æç»“æœ
                comp_analysis = result.get('comprehensive_analysis', {})
                for dim, chinese_name in self.evaluation_dimensions.items():
                    if dim in comp_analysis:
                        df.loc[i, f'llm_{dim}_score'] = comp_analysis[dim].get('score', 0)
                        df.loc[i, f'llm_{dim}_explanation'] = comp_analysis[dim].get('explanation', '')
                
                df.loc[i, 'llm_overall_score'] = comp_analysis.get('overall_score', 0)
                df.loc[i, 'llm_overall_assessment'] = comp_analysis.get('overall_assessment', '')
                
                # ä¸šåŠ¡åˆ†æç»“æœ
                bus_analysis = result.get('business_analysis', {})
                for dim, chinese_name in self.tire_business_dimensions.items():
                    if dim in bus_analysis:
                        df.loc[i, f'llm_business_{dim}_score'] = bus_analysis[dim].get('score', 0)
                        df.loc[i, f'llm_business_{dim}_explanation'] = bus_analysis[dim].get('explanation', '')
                
                df.loc[i, 'llm_business_overall_score'] = bus_analysis.get('business_overall_score', 0)
                
                # å¯¹æ¯”åˆ†æç»“æœ
                comp_analysis = result.get('comparison_analysis', {})
                df.loc[i, 'llm_comparison_winner'] = comp_analysis.get('winner', 'unknown')
                df.loc[i, 'llm_comparison_confidence'] = comp_analysis.get('confidence_level', 'low')
                df.loc[i, 'llm_detailed_analysis'] = comp_analysis.get('detailed_analysis', '')

    def _add_partial_results_to_df(self, df: pd.DataFrame, results: List[Dict], analysis_type: str):
        """
        æ·»åŠ éƒ¨åˆ†ç»“æœåˆ°DataFrameï¼ˆå½“åˆ†æè¢«ä¸­æ–­æ—¶ï¼‰
        
        Args:
            df (pd.DataFrame): æ•°æ®æ¡†
            results (List[Dict]): éƒ¨åˆ†åˆ†æç»“æœ
            analysis_type (str): åˆ†æç±»å‹
        """
        self._add_results_to_df(df, results, analysis_type)
        
        # ä¸ºæœªå®Œæˆçš„è¡Œæ·»åŠ é»˜è®¤å€¼
        for i in range(len(results), len(df)):
            if analysis_type == "comprehensive":
                for dim in self.evaluation_dimensions.keys():
                    df.loc[i, f'llm_{dim}_score'] = 0
                    df.loc[i, f'llm_{dim}_explanation'] = 'åˆ†æè¢«ä¸­æ–­'
                
                df.loc[i, 'llm_overall_score'] = 0
                df.loc[i, 'llm_overall_assessment'] = 'åˆ†æè¢«ä¸­æ–­'

    def generate_analysis_report(self, df: pd.DataFrame) -> Dict:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            df (pd.DataFrame): åŒ…å«åˆ†æç»“æœçš„æ•°æ®æ¡†
            
        Returns:
            Dict: åˆ†ææŠ¥å‘Š
        """
        report = {
            "æ€»æ ·æœ¬æ•°": len(df),
            "åˆ†ææ—¶é—´": datetime.now().isoformat(),
            "ç»¼åˆè¯„ä¼°ç»“æœ": {},
            "ä¸šåŠ¡è¯„ä¼°ç»“æœ": {},
            "å¯¹æ¯”è¯„ä¼°ç»“æœ": {},
            "æ”¹è¿›å»ºè®®": []
        }
        
        # ç»¼åˆè¯„ä¼°ç»Ÿè®¡
        for dim in self.evaluation_dimensions.keys():
            col_name = f'llm_{dim}_score'
            if col_name in df.columns:
                scores = pd.to_numeric(df[col_name], errors='coerce').fillna(0)
                report["ç»¼åˆè¯„ä¼°ç»“æœ"][dim] = {
                    "å¹³å‡åˆ†": scores.mean(),
                    "æœ€é«˜åˆ†": scores.max(),
                    "æœ€ä½åˆ†": scores.min(),
                    "æ ‡å‡†å·®": scores.std()
                }
        
        # ä¸šåŠ¡è¯„ä¼°ç»Ÿè®¡
        for dim in self.tire_business_dimensions.keys():
            col_name = f'llm_business_{dim}_score'
            if col_name in df.columns:
                scores = pd.to_numeric(df[col_name], errors='coerce').fillna(0)
                report["ä¸šåŠ¡è¯„ä¼°ç»“æœ"][dim] = {
                    "å¹³å‡åˆ†": scores.mean(),
                    "æœ€é«˜åˆ†": scores.max(),
                    "æœ€ä½åˆ†": scores.min(),
                    "æ ‡å‡†å·®": scores.std()
                }
        
        # å¯¹æ¯”è¯„ä¼°ç»Ÿè®¡
        if 'llm_comparison_winner' in df.columns:
            winner_counts = df['llm_comparison_winner'].value_counts()
            report["å¯¹æ¯”è¯„ä¼°ç»“æœ"] = {
                "referenceè·èƒœ": winner_counts.get('reference', 0),
                "generatedè·èƒœ": winner_counts.get('generated', 0),
                "å¹³å±€": winner_counts.get('tie', 0),
                "æœªçŸ¥": winner_counts.get('unknown', 0)
            }
        
        return report

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    config = {
        'url': 'https://agents.dyna.ai/openapi/v1/conversation/dialog/',
        'username': 'marshall.ting@dyna.ai',
        'robot_key': 'f79sd16wABIqwLe%2FzjGeZGDRMUo%3D',
        'robot_token': 'MTczODkxMDA0MzUwMgp1cjRVVnF4Y0w3Y2hwRmU3RmxFUXFQV05lSGc9'
    }
    
    analyzer = AdvancedLLMAnalyzer(config)
    
    # æµ‹è¯•æ•°æ®
    reference = '{"type": "markdown", "data": "| ID Producto | Nombre del Producto | Stock | Precio |\\n|:------------|:--------------------|:------|:-------|\\n| LL-C30210 | 185 65 15 COMPASAL BLAZER HP 88H | 8 | $1142 |\\n", "desc": "ğŸ” Resultados de bÃºsqueda\\n\\nğŸ“Š Encontrados: 1 productos\\nğŸ’° Precio: $1142\\n\\nÂ¿CuÃ¡l modelo le interesa?"}'
    generated = '{"type": "markdown", "data": "| ID Producto | Nombre del Producto | Stock | Precio |\\n|:------------|:--------------------|:------|:-------|\\n| LL-C30210 | 185 65 15 COMPASAL BLAZER HP 88H | 8 | $1142 |\\n", "desc": "ğŸ” Resultados de bÃºsqueda\\n\\nğŸ“Š Encontrados: 1 productos\\nğŸ’° Precio: $1142\\n\\nÂ¿CuÃ¡l modelo le interesa?"}'
    
    # æ‰§è¡Œåˆ†æ
    result = analyzer.comprehensive_analyze(reference, generated)
    print("å¢å¼ºLLMåˆ†æç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False)) 